{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#!pip install transformers -U"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T16:55:10.171214Z",
     "start_time": "2024-06-30T16:55:10.139074Z"
    }
   },
   "id": "445de116bad59ad1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:02:44.476300Z",
     "start_time": "2024-06-30T17:02:43.292292Z"
    }
   },
   "id": "ba8b47a55ac570c2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding path: /Users/johnschroter/IdeaProjects/Sigma-GPT/src\n"
     ]
    }
   ],
   "source": [
    "# Add the correct path to the local transformers directory\n",
    "local_path = os.path.abspath('../src/')\n",
    "print(\"Adding path:\", local_path)  # Verify the path to be added\n",
    "sys.path.insert(0, local_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:02:44.476605Z",
     "start_time": "2024-06-30T17:02:44.460639Z"
    }
   },
   "id": "9430a53d8e229b91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating local Path to files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e000b06fe6e35b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confirming local copies are being used"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a54ad731fc72df55"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johnschroter/IdeaProjects/Sigma-GPT/src/transformers/models/gpt2/tokenization_gpt2.py\n",
      "/Users/johnschroter/IdeaProjects/Sigma-GPT/src/transformers/models/gpt2/modeling_gpt2.py\n"
     ]
    }
   ],
   "source": [
    "# Import your modified GPT2 classes\n",
    "from transformers.models.gpt2.tokenization_gpt2 import *\n",
    "from transformers.models.gpt2.modeling_gpt2 import *\n",
    "\n",
    "# Verify that the modules are being loaded from the correct path\n",
    "import transformers.models.gpt2.tokenization_gpt2\n",
    "import transformers.models.gpt2.modeling_gpt2\n",
    "\n",
    "print(transformers.models.gpt2.tokenization_gpt2.__file__)  # Should point to your local file\n",
    "print(transformers.models.gpt2.modeling_gpt2.__file__)  # Should point to your local file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:02:45.922839Z",
     "start_time": "2024-06-30T17:02:45.693233Z"
    }
   },
   "id": "f4f96fca5e604776"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Randomly initilizing sigma-gpt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5b62687d261dc19"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomGPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight', 'wte.LayerNorm.bias', 'wte.LayerNorm.weight', 'wte.next_position_embeddings.weight', 'wte.position_embeddings.weight', 'wte.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer (pre-trained vocab is fine for tokenizer)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Initialize the configuration with random parameters\n",
    "config = GPT2Config()\n",
    "\n",
    "# Initialize the model with the custom configuration\n",
    "#model = CustomGPT2LMHeadModel(config)\n",
    "model = CustomGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize weights randomly\n",
    "#model.init_weights()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:02:49.324723Z",
     "start_time": "2024-06-30T17:02:47.630130Z"
    }
   },
   "id": "f2b440488042861b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "load in dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d3381642b2613f7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f6f9d04af5b4836928b1f4ea0ba183d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3832615818354039ab0219df9f74ef31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Wikitext-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T16:55:25.795730Z",
     "start_time": "2024-06-30T16:55:13.102927Z"
    }
   },
   "id": "dfc23dfd7af026ba"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnschroter/miniforge3/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for ptb_text_only contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ptb_text_only\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Penn Treebank dataset\n",
    "dataset = load_dataset(\"ptb_text_only\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"sentence\"])\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:02:59.756925Z",
     "start_time": "2024-06-30T17:02:51.306731Z"
    }
   },
   "id": "fc7882b63ef14c43"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AdaptiveShuffle:\n",
    "    def __init__(self, initial_shuffle_percentage=0.0, max_adjustment_per_epoch=0.05, performance_threshold=0.25):\n",
    "        self.shuffle_percentage = initial_shuffle_percentage\n",
    "        self.max_adjustment_per_epoch = max_adjustment_per_epoch\n",
    "        self.performance_threshold = performance_threshold\n",
    "        self.previous_loss = None\n",
    "\n",
    "    def adjust_shuffle_percentage(self, current_loss):\n",
    "        if self.previous_loss is not None:\n",
    "            improvement = (self.previous_loss - current_loss) / self.previous_loss\n",
    "            if improvement > self.performance_threshold:\n",
    "                self.shuffle_percentage = min(self.shuffle_percentage + self.max_adjustment_per_epoch, 1.0)\n",
    "            elif improvement < -self.performance_threshold:\n",
    "                self.shuffle_percentage = max(self.shuffle_percentage - self.max_adjustment_per_epoch, 0.0)\n",
    "        self.previous_loss = current_loss\n",
    "\n",
    "    def get_current_shuffle_percentage(self):\n",
    "        return self.shuffle_percentage\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:02:59.763250Z",
     "start_time": "2024-06-30T17:02:59.758985Z"
    }
   },
   "id": "258c15df673fba75"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ShuffledDataset(Dataset):\n",
    "    def __init__(self, input_ids, position_ids, next_position_ids, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.position_ids = position_ids\n",
    "        self.next_position_ids = next_position_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'position_ids': torch.tensor(self.position_ids[idx], dtype=torch.long),\n",
    "            'next_position_ids': torch.tensor(self.next_position_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:02:59.769344Z",
     "start_time": "2024-06-30T17:02:59.763146Z"
    }
   },
   "id": "80196e9cb33294f1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Function to shuffle a percentage of tokens within each sequence\n",
    "def shuffle_with_positional_ids(dataset, shuffle_percentage):\n",
    "    shuffled_input_ids_list = []\n",
    "    shuffled_pos_ids_list = []\n",
    "    next_pos_ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    for example in dataset:\n",
    "        input_ids = example['input_ids']\n",
    "        attention_mask = example['attention_mask']\n",
    "\n",
    "        # Calculate the number of tokens to shuffle\n",
    "        seq_length = len(input_ids)\n",
    "        num_shuffled_tokens = int(seq_length * shuffle_percentage)\n",
    "\n",
    "        # Get indices to shuffle\n",
    "        indices = list(range(seq_length))\n",
    "        indices_to_shuffle = np.random.choice(indices, num_shuffled_tokens, replace=False)\n",
    "\n",
    "        # Create a permutation for the selected indices\n",
    "        permutation = np.random.permutation(num_shuffled_tokens)\n",
    "\n",
    "        # Create shuffled input_ids, pos_ids, and attention_mask\n",
    "        shuffled_input_ids = input_ids.copy()\n",
    "        pos_ids = list(range(seq_length))\n",
    "        shuffled_pos_ids = pos_ids.copy()\n",
    "        shuffled_attention_mask = attention_mask.copy()\n",
    "\n",
    "        for i, idx in enumerate(indices_to_shuffle):\n",
    "            shuffled_input_ids[idx] = input_ids[indices_to_shuffle[permutation[i]]]\n",
    "            shuffled_pos_ids[idx] = pos_ids[indices_to_shuffle[permutation[i]]]\n",
    "            shuffled_attention_mask[idx] = attention_mask[indices_to_shuffle[permutation[i]]]\n",
    "\n",
    "        # Create the next shuffled pos ids\n",
    "        next_pos_ids = shuffled_pos_ids[1:] + [shuffled_pos_ids[0]]\n",
    "\n",
    "        # Append to lists\n",
    "        shuffled_input_ids_list.append(shuffled_input_ids)\n",
    "        shuffled_pos_ids_list.append(shuffled_pos_ids)\n",
    "        next_pos_ids_list.append(next_pos_ids)\n",
    "        attention_mask_list.append(shuffled_attention_mask)\n",
    "\n",
    "    return ShuffledDataset(\n",
    "        shuffled_input_ids_list,\n",
    "        shuffled_pos_ids_list,\n",
    "        next_pos_ids_list,\n",
    "        attention_mask_list\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:02:59.770398Z",
     "start_time": "2024-06-30T17:02:59.768468Z"
    }
   },
   "id": "ec54e37f1a5b82a0"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Shuffle Percentage=0.0\n",
      "Epoch 1, Batch 0, Loss: 13.31872844696045\n",
      "Epoch 1, Batch 1, Loss: 14.655902862548828\n",
      "Epoch 1, Batch 2, Loss: 12.857035636901855\n",
      "Epoch 1, Batch 3, Loss: 8.306848526000977\n",
      "Epoch 1, Batch 4, Loss: 6.957921028137207\n",
      "Epoch 1, Batch 5, Loss: 5.875648021697998\n",
      "Epoch 1: Average Loss=0.09418249927393209, Train Accuracy=0.15473790528873602\n",
      "Epoch 1: Evaluation Loss=0.7126975239447828, Eval Accuracy=0.2641129046678543\n",
      "Epoch 1: Time Taken=32.18552875518799s\n",
      "Epoch 2: Shuffle Percentage=0.0\n",
      "Epoch 2, Batch 0, Loss: 6.114988327026367\n",
      "Epoch 2, Batch 1, Loss: 5.853084087371826\n",
      "Epoch 2, Batch 2, Loss: 6.109198093414307\n",
      "Epoch 2, Batch 3, Loss: 6.391347408294678\n",
      "Epoch 2, Batch 4, Loss: 5.935744285583496\n",
      "Epoch 2, Batch 5, Loss: 5.993216037750244\n",
      "Epoch 2: Average Loss=0.05531546844899836, Train Accuracy=0.2723454336325328\n",
      "Epoch 2: Evaluation Loss=0.6642221594756504, Eval Accuracy=0.2641129046678543\n",
      "Epoch 2: Time Taken=29.603394031524658s\n",
      "Epoch 3: Shuffle Percentage=0.05\n",
      "Epoch 3, Batch 0, Loss: 5.492716312408447\n",
      "Epoch 3, Batch 1, Loss: 5.8529767990112305\n",
      "Epoch 3, Batch 2, Loss: 5.376038551330566\n",
      "Epoch 3, Batch 3, Loss: 5.96901798248291\n",
      "Epoch 3, Batch 4, Loss: 5.529882907867432\n",
      "Epoch 3, Batch 5, Loss: 5.638807773590088\n",
      "Epoch 3: Average Loss=0.051458115998010145, Train Accuracy=0.3020833283662796\n",
      "Epoch 3: Evaluation Loss=0.6385849646802219, Eval Accuracy=0.2641129046678543\n",
      "Epoch 3: Time Taken=31.203632831573486s\n",
      "Epoch 4: Shuffle Percentage=0.05\n",
      "Epoch 4, Batch 0, Loss: 5.444346904754639\n",
      "Epoch 4, Batch 1, Loss: 5.431252479553223\n",
      "Epoch 4, Batch 2, Loss: 5.176545143127441\n",
      "Epoch 4, Batch 3, Loss: 5.01170539855957\n",
      "Epoch 4, Batch 4, Loss: 5.362993240356445\n",
      "Epoch 4, Batch 5, Loss: 4.670949459075928\n",
      "Epoch 4: Average Loss=0.04726108301736664, Train Accuracy=0.3141801059246063\n",
      "Epoch 4: Evaluation Loss=0.593272254152118, Eval Accuracy=0.2641129046678543\n",
      "Epoch 4: Time Taken=39.86124801635742s\n",
      "Epoch 5: Shuffle Percentage=0.05\n",
      "Epoch 5, Batch 0, Loss: 5.250467777252197\n",
      "Epoch 5, Batch 1, Loss: 5.103654861450195\n",
      "Epoch 5, Batch 2, Loss: 5.149396896362305\n",
      "Epoch 5, Batch 3, Loss: 4.868234634399414\n",
      "Epoch 5, Batch 4, Loss: 5.286980152130127\n",
      "Epoch 5, Batch 5, Loss: 4.911983489990234\n",
      "Epoch 5: Average Loss=0.04646005746441409, Train Accuracy=0.2822580660382907\n",
      "Epoch 5: Evaluation Loss=0.5769495154326817, Eval Accuracy=0.28351814299821854\n",
      "Epoch 5: Time Taken=37.132606983184814s\n",
      "Epoch 6: Shuffle Percentage=0.05\n",
      "Epoch 6, Batch 0, Loss: 5.087037086486816\n",
      "Epoch 6, Batch 1, Loss: 4.88749885559082\n",
      "Epoch 6, Batch 2, Loss: 5.18646764755249\n",
      "Epoch 6, Batch 3, Loss: 5.478724956512451\n",
      "Epoch 6, Batch 4, Loss: 5.138181686401367\n",
      "Epoch 6, Batch 5, Loss: 4.757720470428467\n",
      "Epoch 6: Average Loss=0.04640673359114348, Train Accuracy=0.28629032025734585\n",
      "Epoch 6: Evaluation Loss=0.5710521554047207, Eval Accuracy=0.27528562148412067\n",
      "Epoch 6: Time Taken=40.41963815689087s\n",
      "Epoch 7: Shuffle Percentage=0.05\n",
      "Epoch 7, Batch 0, Loss: 4.539489269256592\n",
      "Epoch 7, Batch 1, Loss: 4.786983013153076\n",
      "Epoch 7, Batch 2, Loss: 4.735699653625488\n",
      "Epoch 7, Batch 3, Loss: 4.8730950355529785\n",
      "Epoch 7, Batch 4, Loss: 4.799619674682617\n",
      "Epoch 7, Batch 5, Loss: 4.69767427444458\n",
      "Epoch 7: Average Loss=0.04321057890686221, Train Accuracy=0.3198084682226181\n",
      "Epoch 7: Evaluation Loss=0.5627096913895517, Eval Accuracy=0.2906586031119029\n",
      "Epoch 7: Time Taken=40.32683992385864s\n",
      "Epoch 8: Shuffle Percentage=0.05\n",
      "Epoch 8, Batch 0, Loss: 4.931626796722412\n",
      "Epoch 8, Batch 1, Loss: 5.1712646484375\n",
      "Epoch 8, Batch 2, Loss: 4.698574066162109\n",
      "Epoch 8, Batch 3, Loss: 4.400755405426025\n",
      "Epoch 8, Batch 4, Loss: 5.081347942352295\n",
      "Epoch 8, Batch 5, Loss: 4.8290629386901855\n",
      "Epoch 8: Average Loss=0.04424412127323788, Train Accuracy=0.2978830635547638\n",
      "Epoch 8: Evaluation Loss=0.5534862302384287, Eval Accuracy=0.32165657977263135\n",
      "Epoch 8: Time Taken=41.082212924957275s\n",
      "Epoch 9: Shuffle Percentage=0.05\n",
      "Epoch 9, Batch 0, Loss: 5.048299789428711\n",
      "Epoch 9, Batch 1, Loss: 4.568792343139648\n",
      "Epoch 9, Batch 2, Loss: 4.967745304107666\n",
      "Epoch 9, Batch 3, Loss: 4.893973350524902\n",
      "Epoch 9, Batch 4, Loss: 4.71068000793457\n",
      "Epoch 9, Batch 5, Loss: 4.504857540130615\n",
      "Epoch 9: Average Loss=0.04360843212046522, Train Accuracy=0.32585684955120087\n",
      "Epoch 9: Evaluation Loss=0.5472230461408507, Eval Accuracy=0.3266129046678543\n",
      "Epoch 9: Time Taken=41.134941816329956s\n",
      "Epoch 10: Shuffle Percentage=0.05\n",
      "Epoch 10, Batch 0, Loss: 4.562207221984863\n",
      "Epoch 10, Batch 1, Loss: 5.159724235534668\n",
      "Epoch 10, Batch 2, Loss: 5.00144100189209\n",
      "Epoch 10, Batch 3, Loss: 4.852461338043213\n",
      "Epoch 10, Batch 4, Loss: 4.620960712432861\n",
      "Epoch 10, Batch 5, Loss: 4.626915454864502\n",
      "Epoch 10: Average Loss=0.04380503034156869, Train Accuracy=0.33148521681626636\n",
      "Epoch 10: Evaluation Loss=0.5408271753563071, Eval Accuracy=0.34929434955120087\n",
      "Epoch 10: Time Taken=35.6781370639801s\n",
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def plot_attention_heatmap(attention_matrix, epoch, layer_idx, head_idx):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(attention_matrix, cmap='viridis')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_title(f'Epoch {epoch+1}, Layer {layer_idx+1}, Head {head_idx+1}')\n",
    "    return fig\n",
    "\n",
    "def train_model(model, tokenizer, adaptive_shuffle, train_dataset, eval_dataset, num_epochs=10, batch_size=64, log_dir='./logs'):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    def compute_accuracy(logits, labels):\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct = (preds == labels).float()\n",
    "        return correct.sum() / correct.numel()\n",
    "\n",
    "    # Log the model architecture\n",
    "    dummy_input_ids = torch.randint(0, 100, (1, 64)).to(model.device)\n",
    "    dummy_position_ids = torch.randint(0, 100, (1, 64)).to(model.device)\n",
    "    dummy_next_position_ids = torch.randint(0, 100, (1, 64)).to(model.device)\n",
    "    dummy_attention_mask = torch.ones((1, 64)).to(model.device)\n",
    "    #writer.add_graph(model, (dummy_input_ids, dummy_position_ids, dummy_next_position_ids, dummy_attention_mask))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        shuffle_percentage = adaptive_shuffle.get_current_shuffle_percentage()\n",
    "        print(f\"Epoch {epoch + 1}: Shuffle Percentage={shuffle_percentage}\")\n",
    "\n",
    "        # Shuffle the sequences based on the current shuffle percentage\n",
    "        shuffled_train_dataset = shuffle_with_positional_ids(train_dataset, shuffle_percentage)\n",
    "        train_loader = DataLoader(shuffled_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        count = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Get input and target sequences\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            position_ids = batch['position_ids'].to(model.device)\n",
    "            next_position_ids = batch['next_position_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, position_ids=position_ids, next_position_ids=next_position_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "            logits = outputs.logits\n",
    "            # Compute loss\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Compute accuracy\n",
    "            total_correct += compute_accuracy(shift_logits, shift_labels).item() * shift_labels.numel()\n",
    "            total_samples += shift_labels.numel()\n",
    "\n",
    "\n",
    "            if batch_idx % (1 * 1) == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "            \n",
    "            count = count + 1\n",
    "            if count > 5:\n",
    "                break\n",
    "        # Log gradient norms\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        writer.add_scalar('Gradient Norm/Train', total_norm, epoch)\n",
    "\n",
    "        # Log learning rate\n",
    "        writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch)\n",
    "        # Log attention heatmaps\n",
    "        for layer_idx, layer_attention in enumerate(outputs.attentions):\n",
    "            for head_idx, head_attention in enumerate(layer_attention[0]):\n",
    "                attention_matrix = head_attention.detach().cpu().numpy()\n",
    "                fig = plot_attention_heatmap(attention_matrix, epoch, layer_idx, head_idx)\n",
    "                writer.add_figure(f'Attention/Layer_{layer_idx+1}_Head_{head_idx+1}', fig, epoch)\n",
    "        scheduler.step()\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = total_correct / total_samples\n",
    "        writer.add_scalar('Loss/Train', average_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
    "        \n",
    "        adaptive_shuffle.adjust_shuffle_percentage(average_loss)\n",
    "        print(f\"Epoch {epoch + 1}: Average Loss={average_loss}, Train Accuracy={train_accuracy}\")\n",
    "\n",
    "        # Evaluation part\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_correct = 0\n",
    "        eval_samples = 0\n",
    "        shuffled_eval_dataset = shuffle_with_positional_ids(eval_dataset, shuffle_percentage)\n",
    "        eval_loader = DataLoader(shuffled_eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        eval_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_loader:\n",
    "                input_ids = batch['input_ids'].to(model.device)\n",
    "                position_ids = batch['position_ids'].to(model.device)\n",
    "                next_position_ids = batch['next_position_ids'].to(model.device)\n",
    "                attention_mask = batch['attention_mask'].to(model.device)\n",
    "                labels = input_ids.clone()\n",
    "                outputs = model(input_ids=input_ids, position_ids=position_ids, next_position_ids=next_position_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = input_ids[..., 1:].contiguous()\n",
    "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                eval_loss += loss.item()\n",
    "                eval_correct += compute_accuracy(shift_logits, shift_labels).item() * shift_labels.numel()\n",
    "                eval_samples += shift_labels.numel()\n",
    "\n",
    "                eval_count = eval_count +1\n",
    "                if eval_count > 5:\n",
    "                    break\n",
    "\n",
    "        average_eval_loss = eval_loss / len(eval_loader)\n",
    "        eval_accuracy = eval_correct / eval_samples\n",
    "        writer.add_scalar('Loss/Eval', average_eval_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Eval', eval_accuracy, epoch)\n",
    "        print(f\"Epoch {epoch + 1}: Evaluation Loss={average_eval_loss}, Eval Accuracy={eval_accuracy}\")\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        writer.add_scalar('Time/Epoch', epoch_time, epoch)\n",
    "        print(f\"Epoch {epoch + 1}: Time Taken={epoch_time}s\")\n",
    "\n",
    "        # Log weight and bias histograms\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f\"{name}/weight\", param, epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"{name}/grad\", param.grad, epoch)\n",
    "\n",
    "    print(\"Training completed\")\n",
    "    writer.close()\n",
    "\n",
    "adaptive_shuffle = AdaptiveShuffle()\n",
    "train_model(model, tokenizer, adaptive_shuffle, train_dataset, eval_dataset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:12:12.700337Z",
     "start_time": "2024-06-30T17:02:59.770663Z"
    }
   },
   "id": "a4e123f54a44965"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Shuffle Percentage=0.05\n",
      "Epoch 1, Batch 0, Loss: 5.067913055419922\n",
      "Epoch 1, Batch 1, Loss: 4.939890384674072\n",
      "Epoch 1, Batch 2, Loss: 10.544111251831055\n",
      "Epoch 1, Batch 3, Loss: 4.924056053161621\n",
      "Epoch 1, Batch 4, Loss: 4.949413299560547\n",
      "Epoch 1, Batch 5, Loss: 5.626040935516357\n",
      "Epoch 1: Average Loss=0.054789399665902086, Train Accuracy=0.299563171962897\n",
      "Epoch 1: Evaluation Loss=0.5912486022373415, Eval Accuracy=0.2960349420706431\n",
      "Epoch 1: Time Taken=47.37106800079346s\n",
      "Epoch 2: Shuffle Percentage=0.0\n",
      "Epoch 2, Batch 0, Loss: 5.0094780921936035\n",
      "Epoch 2, Batch 1, Loss: 5.189168453216553\n",
      "Epoch 2, Batch 2, Loss: 4.645810604095459\n",
      "Epoch 2, Batch 3, Loss: 5.051339149475098\n",
      "Epoch 2, Batch 4, Loss: 4.14481258392334\n",
      "Epoch 2, Batch 5, Loss: 5.03223180770874\n",
      "Epoch 2: Average Loss=0.04418364846597689, Train Accuracy=0.3247647782166799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, tokenizer, adaptive_shuffle, train_dataset, eval_dataset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T17:37:47.519451Z",
     "start_time": "2024-06-30T17:36:03.102667Z"
    }
   },
   "id": "f41dbd9171b28d25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Shuffle the sequences based on the current shuffle percentage\n",
    "shuffled_train_dataset = shuffle_with_positional_ids(train_dataset, 0)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(shuffled_train_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.804730Z"
    }
   },
   "id": "93a27b713146ebec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    count = count + 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.805415Z"
    }
   },
   "id": "d26fb8f0ee9c6c68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.806166Z"
    }
   },
   "id": "85afdf7eb8960c2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shuffled_train_dataset[14]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.807055Z"
    }
   },
   "id": "51d608bd1b64d73d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.att"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.807995Z"
    }
   },
   "id": "f3b345b4707cf6c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
