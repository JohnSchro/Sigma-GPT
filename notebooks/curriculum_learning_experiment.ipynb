{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "#!pip install transformers -U"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T16:55:10.171214Z",
     "start_time": "2024-06-30T16:55:10.139074Z"
    }
   },
   "id": "445de116bad59ad1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T18:11:58.999053Z",
     "start_time": "2024-06-30T18:11:58.012801Z"
    }
   },
   "id": "ba8b47a55ac570c2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding path: /Users/johnschroter/IdeaProjects/Sigma-GPT/src\n"
     ]
    }
   ],
   "source": [
    "# Add the correct path to the local transformers directory\n",
    "local_path = os.path.abspath('../src/')\n",
    "print(\"Adding path:\", local_path)  # Verify the path to be added\n",
    "sys.path.insert(0, local_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T18:11:59.421426Z",
     "start_time": "2024-06-30T18:11:59.412270Z"
    }
   },
   "id": "9430a53d8e229b91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating local Path to files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e000b06fe6e35b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confirming local copies are being used"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a54ad731fc72df55"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johnschroter/IdeaProjects/Sigma-GPT/src/transformers/models/gpt2/tokenization_gpt2.py\n",
      "/Users/johnschroter/IdeaProjects/Sigma-GPT/src/transformers/models/gpt2/modeling_gpt2.py\n"
     ]
    }
   ],
   "source": [
    "# Import your modified GPT2 classes\n",
    "from transformers.models.gpt2.tokenization_gpt2 import *\n",
    "from transformers.models.gpt2.modeling_gpt2 import *\n",
    "\n",
    "# Verify that the modules are being loaded from the correct path\n",
    "import transformers.models.gpt2.tokenization_gpt2\n",
    "import transformers.models.gpt2.modeling_gpt2\n",
    "\n",
    "print(transformers.models.gpt2.tokenization_gpt2.__file__)  # Should point to your local file\n",
    "print(transformers.models.gpt2.modeling_gpt2.__file__)  # Should point to your local file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T18:12:01.611459Z",
     "start_time": "2024-06-30T18:12:01.369971Z"
    }
   },
   "id": "f4f96fca5e604776"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Randomly initilizing sigma-gpt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5b62687d261dc19"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomGPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight', 'wte.LayerNorm.bias', 'wte.LayerNorm.weight', 'wte.next_position_embeddings.weight', 'wte.position_embeddings.weight', 'wte.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer (pre-trained vocab is fine for tokenizer)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Initialize the configuration with random parameters\n",
    "config = GPT2Config()\n",
    "\n",
    "# Initialize the model with the custom configuration\n",
    "#model = CustomGPT2LMHeadModel(config)\n",
    "model = CustomGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize weights randomly\n",
    "#model.init_weights()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T18:12:03.820892Z",
     "start_time": "2024-06-30T18:12:02.632964Z"
    }
   },
   "id": "f2b440488042861b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "load in dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d3381642b2613f7"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/4358 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f6f9d04af5b4836928b1f4ea0ba183d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3832615818354039ab0219df9f74ef31"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Wikitext-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T16:55:25.795730Z",
     "start_time": "2024-06-30T16:55:13.102927Z"
    }
   },
   "id": "dfc23dfd7af026ba"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnschroter/miniforge3/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for ptb_text_only contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ptb_text_only\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Penn Treebank dataset\n",
    "dataset = load_dataset(\"ptb_text_only\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"sentence\"])\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T18:12:10.012463Z",
     "start_time": "2024-06-30T18:12:05.979056Z"
    }
   },
   "id": "fc7882b63ef14c43"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AdaptiveShuffle:\n",
    "    def __init__(self, initial_shuffle_percentage=1.0, max_adjustment_per_epoch=0.05, performance_threshold=0.25):\n",
    "        self.shuffle_percentage = initial_shuffle_percentage\n",
    "        self.max_adjustment_per_epoch = max_adjustment_per_epoch\n",
    "        self.performance_threshold = performance_threshold\n",
    "        self.previous_loss = None\n",
    "\n",
    "    def adjust_shuffle_percentage(self, current_loss):\n",
    "        if self.previous_loss is not None:\n",
    "            improvement = (self.previous_loss - current_loss) / self.previous_loss\n",
    "            if improvement > self.performance_threshold:\n",
    "                self.shuffle_percentage = min(self.shuffle_percentage + self.max_adjustment_per_epoch, 1.0)\n",
    "            elif improvement < -self.performance_threshold:\n",
    "                self.shuffle_percentage = max(self.shuffle_percentage - self.max_adjustment_per_epoch, 0.0)\n",
    "        self.previous_loss = current_loss\n",
    "\n",
    "    def get_current_shuffle_percentage(self):\n",
    "        return self.shuffle_percentage\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T18:12:10.017698Z",
     "start_time": "2024-06-30T18:12:10.015746Z"
    }
   },
   "id": "258c15df673fba75"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ShuffledDataset(Dataset):\n",
    "    def __init__(self, input_ids, position_ids, next_position_ids, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.position_ids = position_ids\n",
    "        self.next_position_ids = next_position_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'position_ids': torch.tensor(self.position_ids[idx], dtype=torch.long),\n",
    "            'next_position_ids': torch.tensor(self.next_position_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T18:12:10.022546Z",
     "start_time": "2024-06-30T18:12:10.019528Z"
    }
   },
   "id": "80196e9cb33294f1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Function to shuffle a percentage of tokens within each sequence\n",
    "def shuffle_with_positional_ids(dataset, shuffle_percentage):\n",
    "    shuffled_input_ids_list = []\n",
    "    shuffled_pos_ids_list = []\n",
    "    next_pos_ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    for example in dataset:\n",
    "        input_ids = example['input_ids']\n",
    "        attention_mask = example['attention_mask']\n",
    "\n",
    "        # Calculate the number of tokens to shuffle\n",
    "        seq_length = len(input_ids)\n",
    "        num_shuffled_tokens = int(seq_length * shuffle_percentage)\n",
    "\n",
    "        # Get indices to shuffle\n",
    "        indices = list(range(seq_length))\n",
    "        indices_to_shuffle = np.random.choice(indices, num_shuffled_tokens, replace=False)\n",
    "\n",
    "        # Create a permutation for the selected indices\n",
    "        permutation = np.random.permutation(num_shuffled_tokens)\n",
    "\n",
    "        # Create shuffled input_ids, pos_ids, and attention_mask\n",
    "        shuffled_input_ids = input_ids.copy()\n",
    "        pos_ids = list(range(seq_length))\n",
    "        shuffled_pos_ids = pos_ids.copy()\n",
    "        shuffled_attention_mask = attention_mask.copy()\n",
    "\n",
    "        for i, idx in enumerate(indices_to_shuffle):\n",
    "            shuffled_input_ids[idx] = input_ids[indices_to_shuffle[permutation[i]]]\n",
    "            shuffled_pos_ids[idx] = pos_ids[indices_to_shuffle[permutation[i]]]\n",
    "            shuffled_attention_mask[idx] = attention_mask[indices_to_shuffle[permutation[i]]]\n",
    "\n",
    "        # Create the next shuffled pos ids\n",
    "        next_pos_ids = shuffled_pos_ids[1:] + [shuffled_pos_ids[0]]\n",
    "\n",
    "        # Append to lists\n",
    "        shuffled_input_ids_list.append(shuffled_input_ids)\n",
    "        shuffled_pos_ids_list.append(shuffled_pos_ids)\n",
    "        next_pos_ids_list.append(next_pos_ids)\n",
    "        attention_mask_list.append(shuffled_attention_mask)\n",
    "\n",
    "    return ShuffledDataset(\n",
    "        shuffled_input_ids_list,\n",
    "        shuffled_pos_ids_list,\n",
    "        next_pos_ids_list,\n",
    "        attention_mask_list\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T18:12:10.027463Z",
     "start_time": "2024-06-30T18:12:10.025568Z"
    }
   },
   "id": "ec54e37f1a5b82a0"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Shuffle Percentage=1.0\n",
      "Epoch 1, Batch 0, Loss: 13.54886245727539\n",
      "Epoch 1, Batch 1, Loss: 13.046292304992676\n",
      "Epoch 1, Batch 2, Loss: 12.963492393493652\n",
      "Epoch 1, Batch 3, Loss: 8.204950332641602\n",
      "Epoch 1, Batch 4, Loss: 6.171317100524902\n",
      "Epoch 1, Batch 5, Loss: 6.219213962554932\n",
      "Epoch 1: Average Loss=0.09141964825453366, Train Accuracy=0.14474126455994943\n",
      "Epoch 1: Evaluation Loss=0.7370542490257407, Eval Accuracy=0.25646841526031494\n",
      "Epoch 1: Time Taken=41.761093854904175s\n",
      "Epoch 2: Shuffle Percentage=1.0\n",
      "Epoch 2, Batch 0, Loss: 6.213573455810547\n",
      "Epoch 2, Batch 1, Loss: 6.324640274047852\n",
      "Epoch 2, Batch 2, Loss: 6.55735969543457\n",
      "Epoch 2, Batch 3, Loss: 6.454240322113037\n",
      "Epoch 2, Batch 4, Loss: 6.219867706298828\n",
      "Epoch 2, Batch 5, Loss: 6.089297771453857\n",
      "Epoch 2: Average Loss=0.05753644259142658, Train Accuracy=0.20220094298322996\n",
      "Epoch 2: Evaluation Loss=0.7239555322899008, Eval Accuracy=0.17977150281270346\n",
      "Epoch 2: Time Taken=43.42031502723694s\n",
      "Epoch 3: Shuffle Percentage=1.0\n",
      "Epoch 3, Batch 0, Loss: 6.115586280822754\n",
      "Epoch 3, Batch 1, Loss: 5.59628963470459\n",
      "Epoch 3, Batch 2, Loss: 5.8588385581970215\n",
      "Epoch 3, Batch 3, Loss: 5.836853504180908\n",
      "Epoch 3, Batch 4, Loss: 6.169323444366455\n",
      "Epoch 3, Batch 5, Loss: 5.670145511627197\n",
      "Epoch 3: Average Loss=0.053566925431457336, Train Accuracy=0.27578965077797574\n",
      "Epoch 3: Evaluation Loss=0.6680316745110277, Eval Accuracy=0.2557963728904724\n",
      "Epoch 3: Time Taken=41.35898995399475s\n",
      "Epoch 4: Shuffle Percentage=1.0\n",
      "Epoch 4, Batch 0, Loss: 5.982969760894775\n",
      "Epoch 4, Batch 1, Loss: 5.466378688812256\n",
      "Epoch 4, Batch 2, Loss: 5.844254016876221\n",
      "Epoch 4, Batch 3, Loss: 5.71403694152832\n",
      "Epoch 4, Batch 4, Loss: 5.737512588500977\n",
      "Epoch 4, Batch 5, Loss: 5.198175430297852\n",
      "Epoch 4: Average Loss=0.05158560399226505, Train Accuracy=0.2852822591861089\n",
      "Epoch 4: Evaluation Loss=0.6536825108078291, Eval Accuracy=0.25646841029326123\n",
      "Epoch 4: Time Taken=39.756335973739624s\n",
      "Epoch 5: Shuffle Percentage=1.0\n",
      "Epoch 5, Batch 0, Loss: 5.480077266693115\n",
      "Epoch 5, Batch 1, Loss: 5.57117223739624\n",
      "Epoch 5, Batch 2, Loss: 5.31141996383667\n",
      "Epoch 5, Batch 3, Loss: 5.409070014953613\n",
      "Epoch 5, Batch 4, Loss: 5.9267964363098145\n",
      "Epoch 5, Batch 5, Loss: 5.829164028167725\n",
      "Epoch 5: Average Loss=0.05095395128777687, Train Accuracy=0.28687836478153866\n",
      "Epoch 5: Evaluation Loss=0.6501799259545669, Eval Accuracy=0.256216399371624\n",
      "Epoch 5: Time Taken=40.48913502693176s\n",
      "Epoch 6: Shuffle Percentage=1.0\n",
      "Epoch 6, Batch 0, Loss: 5.420830249786377\n",
      "Epoch 6, Batch 1, Loss: 5.466521263122559\n",
      "Epoch 6, Batch 2, Loss: 5.629421710968018\n",
      "Epoch 6, Batch 3, Loss: 5.639771461486816\n",
      "Epoch 6, Batch 4, Loss: 5.741781711578369\n",
      "Epoch 6, Batch 5, Loss: 5.749777793884277\n",
      "Epoch 6: Average Loss=0.051136936460222514, Train Accuracy=0.2702452937761943\n",
      "Epoch 6: Evaluation Loss=0.6437043243984006, Eval Accuracy=0.25638440748055774\n",
      "Epoch 6: Time Taken=39.91127681732178s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 167\u001B[0m\n\u001B[1;32m    164\u001B[0m     writer\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m    166\u001B[0m adaptive_shuffle \u001B[38;5;241m=\u001B[39m AdaptiveShuffle()\n\u001B[0;32m--> 167\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madaptive_shuffle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[9], line 161\u001B[0m, in \u001B[0;36mtrain_model\u001B[0;34m(model, tokenizer, adaptive_shuffle, train_dataset, eval_dataset, num_epochs, batch_size, log_dir)\u001B[0m\n\u001B[1;32m    159\u001B[0m         writer\u001B[38;5;241m.\u001B[39madd_histogram(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/weight\u001B[39m\u001B[38;5;124m\"\u001B[39m, param, epoch)\n\u001B[1;32m    160\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m param\u001B[38;5;241m.\u001B[39mgrad \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 161\u001B[0m             \u001B[43mwriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_histogram\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mname\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/grad\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepoch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    163\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining completed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    164\u001B[0m writer\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py:485\u001B[0m, in \u001B[0;36mSummaryWriter.add_histogram\u001B[0;34m(self, tag, values, global_step, bins, walltime, max_bins)\u001B[0m\n\u001B[1;32m    482\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(bins, \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m bins \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtensorflow\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    483\u001B[0m     bins \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_bins\n\u001B[1;32m    484\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_file_writer()\u001B[38;5;241m.\u001B[39madd_summary(\n\u001B[0;32m--> 485\u001B[0m     \u001B[43mhistogram\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtag\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbins\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_bins\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_bins\u001B[49m\u001B[43m)\u001B[49m, global_step, walltime\n\u001B[1;32m    486\u001B[0m )\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:358\u001B[0m, in \u001B[0;36mhistogram\u001B[0;34m(name, values, bins, max_bins)\u001B[0m\n\u001B[1;32m    343\u001B[0m \u001B[38;5;124;03m\"\"\"Outputs a `Summary` protocol buffer with a histogram.\u001B[39;00m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;124;03mThe generated\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;124;03m[`Summary`](https://www.tensorflow.org/code/tensorflow/core/framework/summary.proto)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    355\u001B[0m \u001B[38;5;124;03m  buffer.\u001B[39;00m\n\u001B[1;32m    356\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    357\u001B[0m values \u001B[38;5;241m=\u001B[39m make_np(values)\n\u001B[0;32m--> 358\u001B[0m hist \u001B[38;5;241m=\u001B[39m \u001B[43mmake_histogram\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mastype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbins\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_bins\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Summary(value\u001B[38;5;241m=\u001B[39m[Summary\u001B[38;5;241m.\u001B[39mValue(tag\u001B[38;5;241m=\u001B[39mname, histo\u001B[38;5;241m=\u001B[39mhist)])\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:367\u001B[0m, in \u001B[0;36mmake_histogram\u001B[0;34m(values, bins, max_bins)\u001B[0m\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe input has no element.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    366\u001B[0m values \u001B[38;5;241m=\u001B[39m values\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 367\u001B[0m counts, limits \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhistogram\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbins\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbins\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    368\u001B[0m num_bins \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(counts)\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m max_bins \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m num_bins \u001B[38;5;241m>\u001B[39m max_bins:\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mhistogram\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/numpy/lib/histograms.py:878\u001B[0m, in \u001B[0;36mhistogram\u001B[0;34m(a, bins, range, normed, weights, density)\u001B[0m\n\u001B[1;32m    876\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m weights \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    877\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _range(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(a), BLOCK):\n\u001B[0;32m--> 878\u001B[0m         sa \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m:\u001B[49m\u001B[43mi\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43mBLOCK\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    879\u001B[0m         cum_n \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m _search_sorted_inclusive(sa, bin_edges)\n\u001B[1;32m    880\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36msort\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/numpy/core/fromnumeric.py:1004\u001B[0m, in \u001B[0;36msort\u001B[0;34m(a, axis, kind, order)\u001B[0m\n\u001B[1;32m   1002\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1003\u001B[0m     a \u001B[38;5;241m=\u001B[39m asanyarray(a)\u001B[38;5;241m.\u001B[39mcopy(order\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mK\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1004\u001B[0m \u001B[43ma\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msort\u001B[49m\u001B[43m(\u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkind\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkind\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43morder\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m a\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def plot_attention_heatmap(attention_matrix, epoch, layer_idx, head_idx):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    cax = ax.matshow(attention_matrix, cmap='viridis')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_title(f'Epoch {epoch+1}, Layer {layer_idx+1}, Head {head_idx+1}')\n",
    "    return fig\n",
    "\n",
    "def train_model(model, tokenizer, adaptive_shuffle, train_dataset, eval_dataset, num_epochs=10, batch_size=64, log_dir='./logs'):\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    def compute_accuracy(logits, labels):\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct = (preds == labels).float()\n",
    "        return correct.sum() / correct.numel()\n",
    "\n",
    "    # Log the model architecture\n",
    "    dummy_input_ids = torch.randint(0, 100, (1, 64)).to(model.device)\n",
    "    dummy_position_ids = torch.randint(0, 100, (1, 64)).to(model.device)\n",
    "    dummy_next_position_ids = torch.randint(0, 100, (1, 64)).to(model.device)\n",
    "    dummy_attention_mask = torch.ones((1, 64)).to(model.device)\n",
    "    #writer.add_graph(model, (dummy_input_ids, dummy_position_ids, dummy_next_position_ids, dummy_attention_mask))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        shuffle_percentage = adaptive_shuffle.get_current_shuffle_percentage()\n",
    "        print(f\"Epoch {epoch + 1}: Shuffle Percentage={shuffle_percentage}\")\n",
    "\n",
    "        # Shuffle the sequences based on the current shuffle percentage\n",
    "        shuffled_train_dataset = shuffle_with_positional_ids(train_dataset, shuffle_percentage)\n",
    "        train_loader = DataLoader(shuffled_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        # Select the first batch for logging attention heatmaps\n",
    "        first_batch = next(iter(train_loader))\n",
    "        fixed_input_ids = first_batch['input_ids'].to(model.device)\n",
    "        fixed_position_ids = first_batch['position_ids'].to(model.device)\n",
    "        fixed_next_position_ids = first_batch['next_position_ids'].to(model.device)\n",
    "        fixed_attention_mask = first_batch['attention_mask'].to(model.device)\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        count = 0\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Get input and target sequences\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            position_ids = batch['position_ids'].to(model.device)\n",
    "            next_position_ids = batch['next_position_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, position_ids=position_ids, next_position_ids=next_position_ids, attention_mask=attention_mask, output_attentions=True)\n",
    "            logits = outputs.logits\n",
    "            # Compute loss\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Compute accuracy\n",
    "            total_correct += compute_accuracy(shift_logits, shift_labels).item() * shift_labels.numel()\n",
    "            total_samples += shift_labels.numel()\n",
    "\n",
    "\n",
    "            if batch_idx % (1 * 1) == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "            \n",
    "            count = count + 1\n",
    "            if count > 5:\n",
    "                break\n",
    "        # Log gradient norms\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        writer.add_scalar('Gradient Norm/Train', total_norm, epoch)\n",
    "\n",
    "        # Log learning rate\n",
    "        writer.add_scalar('Learning Rate', scheduler.get_last_lr()[0], epoch)\n",
    "        # Log attention heatmaps\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            fixed_outputs = model(input_ids=fixed_input_ids, position_ids=fixed_position_ids, next_position_ids=fixed_next_position_ids, attention_mask=fixed_attention_mask, output_attentions=True)\n",
    "            for layer_idx, layer_attention in enumerate(fixed_outputs.attentions):\n",
    "                for head_idx, head_attention in enumerate(layer_attention[0]):\n",
    "                    attention_matrix = head_attention.detach().cpu().numpy()\n",
    "                    fig = plot_attention_heatmap(attention_matrix, epoch, layer_idx, head_idx)\n",
    "                    writer.add_figure(f'Attention/Layer_{layer_idx+1}_Head_{head_idx+1}', fig, epoch)\n",
    "                    \n",
    "        scheduler.step()\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = total_correct / total_samples\n",
    "        writer.add_scalar('Loss/Train', average_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
    "        \n",
    "        adaptive_shuffle.adjust_shuffle_percentage(average_loss)\n",
    "        print(f\"Epoch {epoch + 1}: Average Loss={average_loss}, Train Accuracy={train_accuracy}\")\n",
    "\n",
    "        # Evaluation part\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_correct = 0\n",
    "        eval_samples = 0\n",
    "        shuffled_eval_dataset = shuffle_with_positional_ids(eval_dataset, shuffle_percentage)\n",
    "        eval_loader = DataLoader(shuffled_eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        eval_count = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_loader:\n",
    "                input_ids = batch['input_ids'].to(model.device)\n",
    "                position_ids = batch['position_ids'].to(model.device)\n",
    "                next_position_ids = batch['next_position_ids'].to(model.device)\n",
    "                attention_mask = batch['attention_mask'].to(model.device)\n",
    "                labels = input_ids.clone()\n",
    "                outputs = model(input_ids=input_ids, position_ids=position_ids, next_position_ids=next_position_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = input_ids[..., 1:].contiguous()\n",
    "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                eval_loss += loss.item()\n",
    "                eval_correct += compute_accuracy(shift_logits, shift_labels).item() * shift_labels.numel()\n",
    "                eval_samples += shift_labels.numel()\n",
    "\n",
    "                eval_count = eval_count +1\n",
    "                if eval_count > 5:\n",
    "                    break\n",
    "\n",
    "        average_eval_loss = eval_loss / len(eval_loader)\n",
    "        eval_accuracy = eval_correct / eval_samples\n",
    "        writer.add_scalar('Loss/Eval', average_eval_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Eval', eval_accuracy, epoch)\n",
    "        print(f\"Epoch {epoch + 1}: Evaluation Loss={average_eval_loss}, Eval Accuracy={eval_accuracy}\")\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        writer.add_scalar('Time/Epoch', epoch_time, epoch)\n",
    "        print(f\"Epoch {epoch + 1}: Time Taken={epoch_time}s\")\n",
    "\n",
    "        # Log weight and bias histograms\n",
    "        for name, param in model.named_parameters():\n",
    "            writer.add_histogram(f\"{name}/weight\", param, epoch)\n",
    "            if param.grad is not None:\n",
    "                writer.add_histogram(f\"{name}/grad\", param.grad, epoch)\n",
    "\n",
    "    print(\"Training completed\")\n",
    "    writer.close()\n",
    "\n",
    "adaptive_shuffle = AdaptiveShuffle()\n",
    "train_model(model, tokenizer, adaptive_shuffle, train_dataset, eval_dataset)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T18:18:01.597482Z",
     "start_time": "2024-06-30T18:12:10.038793Z"
    }
   },
   "id": "a4e123f54a44965"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Shuffle the sequences based on the current shuffle percentage\n",
    "shuffled_train_dataset = shuffle_with_positional_ids(train_dataset, 0)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(shuffled_train_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.804730Z"
    }
   },
   "id": "93a27b713146ebec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count = 0\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    count = count + 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.805415Z"
    }
   },
   "id": "d26fb8f0ee9c6c68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.806166Z"
    }
   },
   "id": "85afdf7eb8960c2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shuffled_train_dataset[14]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.807055Z"
    }
   },
   "id": "51d608bd1b64d73d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.att"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-06-30T16:55:25.807995Z"
    }
   },
   "id": "f3b345b4707cf6c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
