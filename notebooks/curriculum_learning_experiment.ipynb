{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#!pip install transformers -U"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-29T20:04:36.690027Z",
     "start_time": "2024-06-29T20:04:36.666072Z"
    }
   },
   "id": "445de116bad59ad1"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import importlib\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T15:28:54.817502Z",
     "start_time": "2024-06-30T15:28:53.822200Z"
    }
   },
   "id": "ba8b47a55ac570c2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding path: /Users/johnschroter/IdeaProjects/Sigma-GPT/src\n"
     ]
    }
   ],
   "source": [
    "# Add the correct path to the local transformers directory\n",
    "local_path = os.path.abspath('../src/')\n",
    "print(\"Adding path:\", local_path)  # Verify the path to be added\n",
    "sys.path.insert(0, local_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T15:28:54.822070Z",
     "start_time": "2024-06-30T15:28:54.818078Z"
    }
   },
   "id": "9430a53d8e229b91"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating local Path to files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e000b06fe6e35b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confirming local copies are being used"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a54ad731fc72df55"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/johnschroter/IdeaProjects/Sigma-GPT/src/transformers/models/gpt2/tokenization_gpt2.py\n",
      "/Users/johnschroter/IdeaProjects/Sigma-GPT/src/transformers/models/gpt2/modeling_gpt2.py\n"
     ]
    }
   ],
   "source": [
    "# Import your modified GPT2 classes\n",
    "from transformers.models.gpt2.tokenization_gpt2 import *\n",
    "from transformers.models.gpt2.modeling_gpt2 import *\n",
    "\n",
    "# Verify that the modules are being loaded from the correct path\n",
    "import transformers.models.gpt2.tokenization_gpt2\n",
    "import transformers.models.gpt2.modeling_gpt2\n",
    "\n",
    "print(transformers.models.gpt2.tokenization_gpt2.__file__)  # Should point to your local file\n",
    "print(transformers.models.gpt2.modeling_gpt2.__file__)  # Should point to your local file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T15:28:55.294006Z",
     "start_time": "2024-06-30T15:28:55.074964Z"
    }
   },
   "id": "f4f96fca5e604776"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Randomly initilizing sigma-gpt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5b62687d261dc19"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomGPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['lm_head.weight', 'wte.LayerNorm.bias', 'wte.LayerNorm.weight', 'wte.next_position_embeddings.weight', 'wte.position_embeddings.weight', 'wte.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer (pre-trained vocab is fine for tokenizer)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Initialize the configuration with random parameters\n",
    "config = GPT2Config()\n",
    "\n",
    "# Initialize the model with the custom configuration\n",
    "#model = CustomGPT2LMHeadModel(config)\n",
    "model = CustomGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Initialize weights randomly\n",
    "#model.init_weights()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T15:28:57.149231Z",
     "start_time": "2024-06-30T15:28:55.957449Z"
    }
   },
   "id": "f2b440488042861b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "load in dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3d3381642b2613f7"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load Wikitext-2 dataset\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T14:09:51.698284Z",
     "start_time": "2024-06-30T14:09:45.023001Z"
    }
   },
   "id": "dfc23dfd7af026ba"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/johnschroter/miniforge3/lib/python3.10/site-packages/datasets/load.py:1461: FutureWarning: The repository for ptb_text_only contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ptb_text_only\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load Penn Treebank dataset\n",
    "dataset = load_dataset(\"ptb_text_only\")\n",
    "\n",
    "# Preprocess the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=32)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"sentence\"])\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"validation\"]\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T15:29:03.789648Z",
     "start_time": "2024-06-30T15:28:59.752077Z"
    }
   },
   "id": "fc7882b63ef14c43"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AdaptiveShuffle:\n",
    "    def __init__(self, initial_shuffle_percentage=0.0, max_adjustment_per_epoch=0.05, performance_threshold=0.25):\n",
    "        self.shuffle_percentage = initial_shuffle_percentage\n",
    "        self.max_adjustment_per_epoch = max_adjustment_per_epoch\n",
    "        self.performance_threshold = performance_threshold\n",
    "        self.previous_loss = None\n",
    "\n",
    "    def adjust_shuffle_percentage(self, current_loss):\n",
    "        if self.previous_loss is not None:\n",
    "            improvement = (self.previous_loss - current_loss) / self.previous_loss\n",
    "            if improvement > self.performance_threshold:\n",
    "                self.shuffle_percentage = min(self.shuffle_percentage + self.max_adjustment_per_epoch, 1.0)\n",
    "            elif improvement < -self.performance_threshold:\n",
    "                self.shuffle_percentage = max(self.shuffle_percentage - self.max_adjustment_per_epoch, 0.0)\n",
    "        self.previous_loss = current_loss\n",
    "\n",
    "    def get_current_shuffle_percentage(self):\n",
    "        return self.shuffle_percentage\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T15:29:03.794686Z",
     "start_time": "2024-06-30T15:29:03.792445Z"
    }
   },
   "id": "258c15df673fba75"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ShuffledDataset(Dataset):\n",
    "    def __init__(self, input_ids, position_ids, next_position_ids, attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.position_ids = position_ids\n",
    "        self.next_position_ids = next_position_ids\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'position_ids': torch.tensor(self.position_ids[idx], dtype=torch.long),\n",
    "            'next_position_ids': torch.tensor(self.next_position_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_mask[idx], dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T15:29:03.799140Z",
     "start_time": "2024-06-30T15:29:03.795424Z"
    }
   },
   "id": "80196e9cb33294f1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Function to shuffle a percentage of tokens within each sequence\n",
    "def shuffle_with_positional_ids(dataset, shuffle_percentage):\n",
    "    shuffled_input_ids_list = []\n",
    "    shuffled_pos_ids_list = []\n",
    "    next_pos_ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    for example in dataset:\n",
    "        input_ids = example['input_ids']\n",
    "        attention_mask = example['attention_mask']\n",
    "\n",
    "        # Calculate the number of tokens to shuffle\n",
    "        seq_length = len(input_ids)\n",
    "        num_shuffled_tokens = int(seq_length * shuffle_percentage)\n",
    "\n",
    "        # Get indices to shuffle\n",
    "        indices = list(range(seq_length))\n",
    "        indices_to_shuffle = np.random.choice(indices, num_shuffled_tokens, replace=False)\n",
    "\n",
    "        # Create a permutation for the selected indices\n",
    "        permutation = np.random.permutation(num_shuffled_tokens)\n",
    "\n",
    "        # Create shuffled input_ids, pos_ids, and attention_mask\n",
    "        shuffled_input_ids = input_ids.copy()\n",
    "        pos_ids = list(range(seq_length))\n",
    "        shuffled_pos_ids = pos_ids.copy()\n",
    "        shuffled_attention_mask = attention_mask.copy()\n",
    "\n",
    "        for i, idx in enumerate(indices_to_shuffle):\n",
    "            shuffled_input_ids[idx] = input_ids[indices_to_shuffle[permutation[i]]]\n",
    "            shuffled_pos_ids[idx] = pos_ids[indices_to_shuffle[permutation[i]]]\n",
    "            shuffled_attention_mask[idx] = attention_mask[indices_to_shuffle[permutation[i]]]\n",
    "\n",
    "        # Create the next shuffled pos ids\n",
    "        next_pos_ids = shuffled_pos_ids[1:] + [shuffled_pos_ids[0]]\n",
    "\n",
    "        # Append to lists\n",
    "        shuffled_input_ids_list.append(shuffled_input_ids)\n",
    "        shuffled_pos_ids_list.append(shuffled_pos_ids)\n",
    "        next_pos_ids_list.append(next_pos_ids)\n",
    "        attention_mask_list.append(shuffled_attention_mask)\n",
    "\n",
    "    return ShuffledDataset(\n",
    "        shuffled_input_ids_list,\n",
    "        shuffled_pos_ids_list,\n",
    "        next_pos_ids_list,\n",
    "        attention_mask_list\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T15:29:03.803745Z",
     "start_time": "2024-06-30T15:29:03.800932Z"
    }
   },
   "id": "ec54e37f1a5b82a0"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Shuffle Percentage=0.0\n",
      "Epoch 1, Batch 0, Loss: 14.431927680969238\n",
      "Epoch 1, Batch 1, Loss: 11.867571830749512\n",
      "Epoch 1, Batch 2, Loss: 10.281754493713379\n",
      "Epoch 1, Batch 3, Loss: 7.948977947235107\n",
      "Epoch 1, Batch 4, Loss: 6.577035427093506\n",
      "Epoch 1, Batch 5, Loss: 6.500138759613037\n",
      "Epoch 1, Batch 6, Loss: 5.947820663452148\n",
      "Epoch 1, Batch 7, Loss: 6.110809803009033\n",
      "Epoch 1, Batch 8, Loss: 6.134476184844971\n",
      "Epoch 1, Batch 9, Loss: 5.6920390129089355\n",
      "Epoch 1, Batch 10, Loss: 5.871313095092773\n",
      "Epoch 1, Batch 11, Loss: 5.872975826263428\n",
      "Epoch 1, Batch 12, Loss: 5.799072265625\n",
      "Epoch 1, Batch 13, Loss: 5.499769687652588\n",
      "Epoch 1, Batch 14, Loss: 5.837723731994629\n",
      "Epoch 1, Batch 15, Loss: 5.473331928253174\n",
      "Epoch 1, Batch 16, Loss: 5.159011363983154\n",
      "Epoch 1, Batch 17, Loss: 5.7526984214782715\n",
      "Epoch 1, Batch 18, Loss: 5.36810827255249\n",
      "Epoch 1, Batch 19, Loss: 4.941473960876465\n",
      "Epoch 1, Batch 20, Loss: 4.997711658477783\n",
      "Epoch 1, Batch 21, Loss: 5.103751182556152\n",
      "Epoch 1, Batch 22, Loss: 5.188586235046387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def train_model(model, tokenizer, adaptive_shuffle, train_dataset, eval_dataset, num_epochs=10, batch_size=64, log_dir='./logs'):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    def compute_accuracy(logits, labels):\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct = (preds == labels).float()\n",
    "        return correct.sum() / correct.numel()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        shuffle_percentage = adaptive_shuffle.get_current_shuffle_percentage()\n",
    "        print(f\"Epoch {epoch + 1}: Shuffle Percentage={shuffle_percentage}\")\n",
    "\n",
    "        # Shuffle the sequences based on the current shuffle percentage\n",
    "        shuffled_train_dataset = shuffle_with_positional_ids(train_dataset, shuffle_percentage)\n",
    "        train_loader = DataLoader(shuffled_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            # Get input and target sequences\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            position_ids = batch['position_ids'].to(model.device)\n",
    "            next_position_ids = batch['next_position_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "            labels = input_ids.clone()\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, position_ids=position_ids, next_position_ids=next_position_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            # Compute loss\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = input_ids[..., 1:].contiguous()\n",
    "            loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            # Compute accuracy\n",
    "            total_correct += compute_accuracy(shift_logits, shift_labels).item() * shift_labels.numel()\n",
    "            total_samples += shift_labels.numel()\n",
    "            writer.add_scalar('Loss/Train', loss, batch_idx)\n",
    "\n",
    "            if batch_idx % (1 * 1) == 0:\n",
    "                print(f\"Epoch {epoch + 1}, Batch {batch_idx}, Loss: {loss.item()}\")\n",
    "                \n",
    "        scheduler.step()\n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = total_correct / total_samples\n",
    "        writer.add_scalar('Loss/Train', average_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Train', train_accuracy, epoch)\n",
    "        adaptive_shuffle.adjust_shuffle_percentage(average_loss)\n",
    "        print(f\"Epoch {epoch + 1}: Average Loss={average_loss}, Train Accuracy={train_accuracy}\")\n",
    "\n",
    "        # Evaluation part\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_correct = 0\n",
    "        eval_samples = 0\n",
    "        shuffled_eval_dataset = shuffle_with_positional_ids(eval_dataset, shuffle_percentage)\n",
    "        eval_loader = DataLoader(shuffled_eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for batch in eval_loader:\n",
    "                input_ids = batch['input_ids'].to(model.device)\n",
    "                position_ids = batch['position_ids'].to(model.device)\n",
    "                next_position_ids = batch['next_position_ids'].to(model.device)\n",
    "                attention_mask = batch['attention_mask'].to(model.device)\n",
    "                labels = input_ids.clone()\n",
    "                outputs = model(input_ids=input_ids, position_ids=position_ids, next_position_ids=next_position_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                shift_logits = logits[..., :-1, :].contiguous()\n",
    "                shift_labels = input_ids[..., 1:].contiguous()\n",
    "                loss = loss_fn(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "                eval_loss += loss.item()\n",
    "                eval_correct += compute_accuracy(shift_logits, shift_labels).item() * shift_labels.numel()\n",
    "                eval_samples += shift_labels.numel()\n",
    "\n",
    "        average_eval_loss = eval_loss / len(eval_loader)\n",
    "        eval_accuracy = eval_correct / eval_samples\n",
    "        writer.add_scalar('Loss/Eval', average_eval_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Eval', eval_accuracy, epoch)\n",
    "        print(f\"Epoch {epoch + 1}: Evaluation Loss={average_eval_loss}, Eval Accuracy={eval_accuracy}\")\n",
    "\n",
    "        epoch_time = time.time() - start_time\n",
    "        writer.add_scalar('Time/Epoch', epoch_time, epoch)\n",
    "        print(f\"Epoch {epoch + 1}: Time Taken={epoch_time}s\")\n",
    "\n",
    "    print(\"Training completed\")\n",
    "    writer.close()\n",
    "\n",
    "adaptive_shuffle = AdaptiveShuffle()\n",
    "train_model(model, tokenizer, adaptive_shuffle, train_dataset, eval_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T15:30:26.802259Z",
     "start_time": "2024-06-30T15:29:03.803013Z"
    }
   },
   "id": "a4e123f54a44965"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Shuffle the sequences based on the current shuffle percentage\n",
    "shuffled_train_dataset = shuffle_with_positional_ids(train_dataset, 0)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(shuffled_train_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T14:41:27.154589Z",
     "start_time": "2024-06-30T14:41:25.033604Z"
    }
   },
   "id": "93a27b713146ebec"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "count = 0\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    count = count + 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T14:41:27.661688Z",
     "start_time": "2024-06-30T14:41:27.155443Z"
    }
   },
   "id": "d26fb8f0ee9c6c68"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "1315"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T14:41:27.666825Z",
     "start_time": "2024-06-30T14:41:27.661921Z"
    }
   },
   "id": "85afdf7eb8960c2d"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([ 1169,  1279,  2954,    29, 12262,   531, 45450,   373,   973,   287,\n           845, 12949,  6867,   287,  1642,  3348,   329,   262, 16628,   287,\n           262,  1903, 11445,    82,   290,  6928,   351,   257,  1180,  2099,\n           286,  1279,  2954,    29,   287,   399, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256]),\n 'position_ids': tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n         18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n         36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n         54, 55, 56, 57, 58, 59, 60, 61, 62, 63]),\n 'next_position_ids': tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n         19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n         37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n         55, 56, 57, 58, 59, 60, 61, 62, 63,  0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_train_dataset[14]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-30T14:38:40.632096Z",
     "start_time": "2024-06-30T14:38:40.628369Z"
    }
   },
   "id": "51d608bd1b64d73d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.att"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3b345b4707cf6c0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
